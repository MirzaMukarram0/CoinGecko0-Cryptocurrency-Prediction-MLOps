# ============================================================================
# TEST BRANCH - MODEL RETRAINING WITH CML (Dev -> Test)
# ============================================================================
# Runs on PRs from dev to test branch
# Triggers the Airflow DAG to run a full data and model training pipeline
# Uses CML to generate metric comparison reports in PR comments
# Blocks merge if new model performs worse than production model

name: Test - Model Retraining with CML

on:
  pull_request:
    branches:
      - test
  push:
    branches:
      - test

# Required permissions for CML to post comments on PRs
permissions:
  contents: read
  pull-requests: write
  issues: write

env:
  PYTHONPATH: ${{ github.workspace }}
  MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
  MLFLOW_TRACKING_USERNAME: ${{ secrets.MLFLOW_TRACKING_USERNAME }}
  MLFLOW_TRACKING_PASSWORD: ${{ secrets.MLFLOW_TRACKING_PASSWORD }}
  COINGECKO_API_KEY: ${{ secrets.COINGECKO_API_KEY }}

jobs:
  # =========================================================================
  # Job 1: Trigger Airflow DAG for Full Pipeline
  # =========================================================================
  airflow-dag-pipeline:
    name: Airflow DAG Pipeline
    runs-on: ubuntu-latest
    outputs:
      new_model_rmse: ${{ steps.get_metrics.outputs.new_rmse }}
      prod_model_rmse: ${{ steps.compare.outputs.prod_rmse }}
      model_improved: ${{ steps.compare.outputs.model_improved }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Create .env file for Docker
        run: |
          cat > .env << EOF
          COINGECKO_API_KEY=${{ secrets.COINGECKO_API_KEY }}
          MLFLOW_TRACKING_URI=${{ secrets.MLFLOW_TRACKING_URI }}
          MLFLOW_TRACKING_USERNAME=${{ secrets.MLFLOW_TRACKING_USERNAME }}
          MLFLOW_TRACKING_PASSWORD=${{ secrets.MLFLOW_TRACKING_PASSWORD }}
          EOF

      - name: Create data directories
        run: |
          mkdir -p data/raw data/processed data/models data/profiles data/dvc_files
          chmod -R 777 data/

      - name: Start Airflow infrastructure
        run: |
          echo "Starting Airflow infrastructure with Docker Compose..."
          docker compose up -d postgres minio minio-init
          
          # Wait for postgres to be ready
          echo "Waiting for PostgreSQL..."
          sleep 15
          
          # Initialize Airflow
          echo "Initializing Airflow..."
          docker compose up airflow-init
          
          # Start Airflow services
          echo "Starting Airflow scheduler and webserver..."
          docker compose up -d airflow-scheduler airflow-webserver
          
          # Wait for Airflow to be ready
          echo "Waiting for Airflow to be ready..."
          sleep 45
          
          # Check if services are running
          docker compose ps

      - name: Trigger Airflow DAG
        run: |
          echo "Triggering currency_prediction_dag..."
          
          # Wait for webserver health
          MAX_RETRIES=12
          RETRY_COUNT=0
          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            if curl -sf http://localhost:8080/health > /dev/null 2>&1; then
              echo "Airflow webserver is healthy!"
              break
            fi
            RETRY_COUNT=$((RETRY_COUNT + 1))
            echo "Waiting for Airflow webserver... (attempt $RETRY_COUNT/$MAX_RETRIES)"
            sleep 10
          done
          
          # Unpause the DAG first
          echo "Unpausing DAG..."
          curl -X PATCH "http://localhost:8080/api/v1/dags/currency_prediction_dag" \
            -H "Content-Type: application/json" \
            -u "admin:admin" \
            -d '{"is_paused": false}' || true
          
          sleep 5
          
          # Trigger the DAG
          echo "Triggering DAG run..."
          TRIGGER_RESPONSE=$(curl -X POST "http://localhost:8080/api/v1/dags/currency_prediction_dag/dagRuns" \
            -H "Content-Type: application/json" \
            -u "admin:admin" \
            -d "{\"dag_run_id\": \"ci_run_$(date +%Y%m%d_%H%M%S)\", \"conf\": {}}")
          
          echo "Trigger response: $TRIGGER_RESPONSE"
          
          # Extract dag_run_id
          DAG_RUN_ID=$(echo $TRIGGER_RESPONSE | python3 -c "import sys, json; print(json.load(sys.stdin).get('dag_run_id', 'unknown'))")
          echo "DAG Run ID: $DAG_RUN_ID"
          echo "dag_run_id=$DAG_RUN_ID" >> $GITHUB_ENV

      - name: Wait for DAG completion
        run: |
          echo "Waiting for DAG to complete..."
          DAG_RUN_ID="${{ env.dag_run_id }}"
          
          MAX_WAIT=600  # 10 minutes max
          ELAPSED=0
          INTERVAL=15
          
          while [ $ELAPSED -lt $MAX_WAIT ]; do
            STATUS_RESPONSE=$(curl -s "http://localhost:8080/api/v1/dags/currency_prediction_dag/dagRuns/$DAG_RUN_ID" \
              -u "admin:admin")
            
            STATE=$(echo $STATUS_RESPONSE | python3 -c "import sys, json; print(json.load(sys.stdin).get('state', 'unknown'))" 2>/dev/null || echo "unknown")
            
            echo "DAG state: $STATE (elapsed: ${ELAPSED}s)"
            
            if [ "$STATE" = "success" ]; then
              echo "DAG completed successfully!"
              break
            elif [ "$STATE" = "failed" ]; then
              echo "DAG failed!"
              # Get task instance logs
              docker compose logs airflow-scheduler --tail=100
              exit 1
            fi
            
            sleep $INTERVAL
            ELAPSED=$((ELAPSED + INTERVAL))
          done
          
          if [ $ELAPSED -ge $MAX_WAIT ]; then
            echo "DAG did not complete within timeout"
            docker compose logs airflow-scheduler --tail=100
            exit 1
          fi

      - name: Get model metrics from DAG output
        id: get_metrics
        run: |
          echo "Extracting model metrics..."
          
          # Check for training summary from DAG
          if [ -f "data/models/training_summary.json" ]; then
            echo "Found training_summary.json from DAG"
            cat data/models/training_summary.json
            
            python3 << 'EOF'
          import json
          import os
          
          with open('data/models/training_summary.json', 'r') as f:
              summary = json.load(f)
          
          # Get best model metrics
          best_model = summary.get('best_model', {})
          rmse = best_model.get('test_rmse', 99999)
          mae = best_model.get('test_mae', 99999)
          r2 = best_model.get('test_r2', 0)
          model_name = best_model.get('model_name', 'unknown')
          
          # Save for comparison
          with open('new_model_metrics.json', 'w') as f:
              json.dump({
                  'best_model': model_name,
                  'rmse': rmse,
                  'mae': mae,
                  'r2': r2
              }, f, indent=2)
          
          print(f"Best model: {model_name}, RMSE: {rmse:.4f}")
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"new_rmse={rmse:.4f}\n")
          EOF
          else
            echo "Warning: training_summary.json not found, using fallback metrics"
            # Fallback - extract from MLflow or use placeholder
            echo '{"best_model": "unknown", "rmse": 99999, "mae": 99999, "r2": 0}' > new_model_metrics.json
            echo "new_rmse=99999" >> $GITHUB_OUTPUT
          fi

      - name: Compare with production model
        id: compare
        run: |
          echo "Comparing with production model..."
          python3 << 'EOF'
          import os
          import json
          
          try:
              import mlflow
              from mlflow.tracking import MlflowClient
              
              tracking_uri = os.environ.get('MLFLOW_TRACKING_URI', '')
              prod_rmse = 99999.0
              
              if tracking_uri:
                  try:
                      mlflow.set_tracking_uri(tracking_uri)
                      client = MlflowClient()
                      
                      # Try to get best run from experiments
                      experiments = client.search_experiments()
                      for exp in experiments:
                          if exp.name != "Default":
                              runs = client.search_runs(
                                  experiment_ids=[exp.experiment_id],
                                  order_by=["metrics.test_rmse ASC"],
                                  max_results=1
                              )
                              if runs and 'test_rmse' in runs[0].data.metrics:
                                  prod_rmse = float(runs[0].data.metrics['test_rmse'])
                                  print(f"Found production model RMSE: {prod_rmse:.4f}")
                                  break
                  except Exception as e:
                      print(f"Could not connect to MLflow: {e}")
          except ImportError:
              print("MLflow not available, using default production RMSE")
              prod_rmse = 99999.0
          
          # Load new model metrics
          with open('new_model_metrics.json', 'r') as f:
              new_metrics = json.load(f)
          
          new_rmse = new_metrics['rmse']
          
          # Allow 5% tolerance
          model_improved = new_rmse <= prod_rmse * 1.05
          
          print(f"Production RMSE: {prod_rmse:.4f}")
          print(f"New Model RMSE: {new_rmse:.4f}")
          print(f"Model Improved: {model_improved}")
          
          # Save comparison
          with open('model_comparison.json', 'w') as f:
              json.dump({
                  'production_rmse': prod_rmse,
                  'new_rmse': new_rmse,
                  'model_improved': model_improved
              }, f, indent=2)
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"prod_rmse={prod_rmse:.4f}\n")
              f.write(f"model_improved={'true' if model_improved else 'false'}\n")
          EOF

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: training-artifacts
          path: |
            new_model_metrics.json
            model_comparison.json
            data/models/

      - name: Stop Airflow infrastructure
        if: always()
        run: |
          echo "Stopping Airflow infrastructure..."
          docker compose down -v || true

  # =========================================================================
  # Job 2: CML Report Generation
  # =========================================================================
  cml-report:
    name: CML Metric Report
    runs-on: ubuntu-latest
    needs: airflow-dag-pipeline
    if: github.event_name == 'pull_request'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup CML
        uses: iterative/setup-cml@v2

      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          name: training-artifacts

      - name: Generate CML Report
        env:
          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          cat << 'EOF' > report.md
          # Model Performance Comparison Report
          
          ## Summary
          
          **Pipeline:** Airflow DAG (Full Data & Model Training Pipeline)
          
          EOF
          
          python3 << 'PYTHON' >> report.md
          import json
          
          with open('model_comparison.json', 'r') as f:
              data = json.load(f)
          
          if data['model_improved']:
              print("**Status: PASSED** ✅ - New model meets performance requirements")
          else:
              print("**Status: FAILED** ❌ - New model performs worse than production")
          
          print("")
          print("## Metrics Comparison")
          print("")
          print("| Metric | Production Model | New Model | Diff |")
          print("|--------|------------------|-----------|------|")
          diff = data['new_rmse'] - data['production_rmse']
          diff_pct = (diff / data['production_rmse'] * 100) if data['production_rmse'] > 0 else 0
          print(f"| RMSE | {data['production_rmse']:.4f} | {data['new_rmse']:.4f} | {diff:+.4f} ({diff_pct:+.2f}%) |")
          
          with open('new_model_metrics.json', 'r') as f:
              new_data = json.load(f)
          
          print("")
          print("## New Model Details")
          print("")
          print(f"- **Best Model Type:** {new_data['best_model']}")
          print(f"- **RMSE:** {new_data['rmse']:.4f}")
          print(f"- **MAE:** {new_data.get('mae', 'N/A')}")
          print(f"- **R²:** {new_data.get('r2', 'N/A')}")
          PYTHON
          
          cml comment create report.md

      - name: Check model gate
        if: needs.airflow-dag-pipeline.outputs.model_improved == 'false'
        run: |
          echo "Model performance check FAILED"
          echo "New model performs worse than production model"
          echo "Merge is BLOCKED until model performance improves"
          exit 1

  # =========================================================================
  # Job 3: Integration Tests
  # =========================================================================
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: airflow-dag-pipeline
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-ci.txt

      - name: Run Integration Tests
        run: |
          mkdir -p data/raw data/processed data/models data/profiles
          
          python -c "
          import sys
          sys.path.insert(0, '.')
          
          from src.data.extract import CoinGeckoExtractor
          print('CoinGeckoExtractor import passed')
          
          from src.models.train import CryptoPricePredictor
          print('CryptoPricePredictor import passed')
          
          from src.api.app import app
          print('FastAPI app import passed')
          
          print('All integration tests passed!')
          "

      - name: Validate DAG Syntax
        run: |
          python -c "
          import ast
          with open('airflow/dags/currency_prediction_dag.py', 'r') as f:
              ast.parse(f.read())
          print('DAG syntax is valid')
          "
